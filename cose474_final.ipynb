{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOfbzUhai1DdAywvpMIIoqr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install -q transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wK2z5YI2kB8u","executionInfo":{"status":"ok","timestamp":1700030909683,"user_tz":-540,"elapsed":922,"user":{"displayName":"이태후","userId":"03468433647806446322"}},"outputId":"71eb3b5d-b42d-4fa1-b678-cef89471aaab"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["from transformers import AutoModel\n","\n","pretrained = 'dmis-lab/biobert-large-cased-v1.1-squad'\n","model = AutoModel.from_pretrained(pretrained)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Aj9Nzni2kpcE","executionInfo":{"status":"ok","timestamp":1700030929085,"user_tz":-540,"elapsed":19403,"user":{"displayName":"이태후","userId":"03468433647806446322"}},"outputId":"529d2fbf-a2dd-46f4-92a4-e24afec5c34a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertModel were not initialized from the model checkpoint at dmis-lab/biobert-large-cased-v1.1-squad and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["from absl import flags\n","\n","flags.DEFINE_string(\"bert_config_file\", None, \"config json file corresponding to the pretrained BERT model\")\n","flags.DEFINE_string(\"vocab_File\", None, \"vocab file on which BERT was trained\")\n","flags.DEFINE_string(\"output_dir\", None, \"output dir for model checkpoints\")\n","flags.DEFINE_string(\"train_file\", None, \"squad-formatted json for training. E.g., train-v1.1.json\")\n","flags.DEFINE_string(\"predict_file\", None, \"squad-formatted json for predictions. E.g., dev(test)-v1.1.json\")\n","flags.DEFINE_string(\"init_checkpoint\", None, \"initial checkpoint (usually from a pretrained BERT model)\")\n","\n","flags.DEFINE_bool(\"do_lower_case\", True, \"whether to lower-case input text. True for lower case.\")\n","flags.DEFINE_bool(\"do_train\", True, \"whether to run training\")\n","flags.DEFINE_bool(\"do_predict\", True, \"whether to run eval on the dev set\")\n","flags.DEFINE_bool(\"use_tpu\", False, \"whether to use TPU or GPU/CPU\")\n","flags.DEFINE_bool(\"verbose_logging\", False, \"whether to print all warnings during data processing. Other warnings are printed by default.\")\n","\n","flags.DEFINE_integer(\"max_seq_len\", 384, \"maximum input sequence after WordPiece tokenization. Will be padded if shorter, truncated if longer.\")\n","flags.DEFINE_integer(\"doc_stride\", 128, \"when splitting up a long document into chunks, how much stride to take between chunks\")\n","flags.DEFINE_integer(\"max_query_len\", 64, \"maximum query sequence after WordPiece tokenization. Will be truncated if longer.\")\n","flags.DEFINE_integer(\"predict_batch_size\", 8, \"total batch size for predictions\")\n","flags.DEFINE_integer(\"save_checkpoint_step\", 1000, \"how often to save model checkpoints\")\n","flags.DEFINE_integer(\"iterations_per_loop\", 1000, \"how many steps to make in each estimator call\")\n","flags.DEFINE_integer(\"n_best_size\", 20, \"how many n-best predictions to generate in nbest_predictions.json output file\")\n","flags.DEFINE_integer(\"max_answer_len\", 30, \"maximum length of generated answer\")\n","\n","flags.DEFINE_float(\"learning_rate\", 5e-5, \"initial learning rate for Adam optimizer\")\n","flags.DEFINE_float(\"num_train_epochs\", 3.0, \"number of training epochs\")\n","flags.DEFINE_float(\"warmup_proportion\", 0.1, \"proportion of training for linear lr warmup. E.g., 0.1 refers to 10% of training\")\n","flags.DEFINE_float(\"null_score_diff_threshold\", 0.0, \"to predict null if (null_score - best_non_null) > threshold\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M6iOQ_ARlZpZ","executionInfo":{"status":"ok","timestamp":1700030929085,"user_tz":-540,"elapsed":4,"user":{"displayName":"이태후","userId":"03468433647806446322"}},"outputId":"ff2107be-bbc8-40bc-92e2-155f0d3fd176"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<absl.flags._flagvalues.FlagHolder at 0x7acdaff90e20>"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["import sys\n","sys.path.insert(1, '/content/drive/MyDrive/biobert')"],"metadata":{"id":"5INmE5aiyn-b","executionInfo":{"status":"ok","timestamp":1700031854499,"user_tz":-540,"elapsed":1049,"user":{"displayName":"이태후","userId":"03468433647806446322"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import tokenization\n","\n","class SquadExample(object):\n","  def __init__(self, qas_id, question_text, doc_tokens, origin_answer_text=None, start_pos=None, end_pos=None, is_impossible=False):\n","    self.qas_id = qas_id\n","    self.question_text = question_text\n","    self.doc_tokens = doc_tokens\n","    self.origin_answer_text = origin_answer_text\n","    self.start_pos = start_pos\n","    self.end_pos = end_pos\n","    self.is_impossible = is_impossible\n","\n","  def __str__(self):\n","    return self.__repr__()\n","\n","  def __repr__(self):\n","    s = f\"qas_id: {tokenization.printable_text(self.qas_id)}\" # check if text is in str, bytes or unicode and convert\n","    s += f\", question_text: {tokenization.printable_text(self.question_text)}\"\n","    s += f\", doc_tokens: {[' '.join(self.doc_tokens)]}\"\n","\n","    if self.start_pos:\n","      s += f\", start_pos: {self.start_pos}, end_pos: {self.end_pos}, is_impossible: {self.is_impossible}\"\n","\n","    return s\n","\n"],"metadata":{"id":"CAu4TqjuvWVJ","executionInfo":{"status":"ok","timestamp":1700033688783,"user_tz":-540,"elapsed":944,"user":{"displayName":"이태후","userId":"03468433647806446322"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["import json\n","import tensorflow as tf\n","\n","class InputFeatures(object):\n","  def __init(self, unique_id, example_index, doc_span_index, tokens, token_to_origin_map, token_is_max_context,\n","             input_ids, input_mask, segment_ids, start_pos=None, end_pos=None, is_impossible=None):\n","    self.unique_id = unique_id\n","    self.example_index = example_index\n","    self.doc_span_index = doc_span_index\n","    self.tokens = tokens\n","    self.token_to_origin_map = token_to_origin_map\n","    self.token_is_max_context = token_is_max_context\n","    self.input_ids = input_ids\n","    self.input_mask = input_mask\n","    self.segment_ids = segment_ids\n","    self.start_pos = start_pos\n","    self.end_pos = end_pos\n","    self.is_impossible = is_impossible\n","\n","  def read_squad_examples(input_file, is_training):\n","    \"\"\"Read a squad json file into a list of SquadExample\"\"\"\n","\n","    def is_whitespace(c):\n","      if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n","        return True\n","      else:\n","        return False\n","\n","    with open(input_file, \"r\") as rf:\n","      input_data = json.load(rf)[\"data\"][0][\"paragraphs\"]\n","\n","    examples = []\n","    for entry in input_data:\n","      doc_tokens = []\n","      char_to_word_offset = []\n","      prev_is_whitespace = True\n","\n","      context = entry[\"context\"]\n","      for c in context:\n","        if is_whitespace(c):\n","          prev_is_whitespace=True\n","        else:\n","          if prev_is_whitespace: # c comes after whitespace -> add as new token\n","            doc_tokens.append(c)\n","          else:\n","            doc_tokens[-1] += c # c comes after c -> attach to recent token\n","          prev_is_whitespace=False\n","        char_to_word_offset.append(len(doc_tokens) -1) # \"I went home\" -> [0,0,1,1,1,1,1,2,2,2,2]\n","\n","      qas = entry[\"qas\"]\n","      for qa in qas:\n","        qas_id = qa[\"id\"]\n","        question_text = qa[\"question\"]\n","        start_pos = None\n","        end_pos = None\n","        origin_answer_text = None\n","        is_impossible = False\n","\n","        if is_training:\n","          if len(qa[\"answers\"]) != 1:\n","            raise ValueError(\"Each question should have exactly 1 answer.\")\n","\n","          answer = qa[\"answers\"][0]\n","          origin_answer_text = answer[\"text\"] #\"Bazex syndrome\"\n","          answer_offset = answer[\"answer_start\"] #93\n","          answer_length = len(origin_answer_text)\n","          start_pos = char_to_word_offset[answer_offset] # 몇번째 단어부터\n","          end_pos = char_to_word_offset[answer_offset + answer_length-1] # 몇번째 단어까지\n","\n","          # 정답이 context로부터 extract한게 맞는지 확인 (아닌 경우 건너뜀)\n","          answer_from_context = \" \".join(doc_tokens[start_pos:end_pos+1]) # context에서 주어진 범위로 인덱싱한 정답\n","          cleaned_answer = \" \".join(tokenization.whitespace_tokenize(origin_answer_text)) # 실제 정답\n","\n","          if answer_from_context.find(cleaned_answer) == -1: # context에 정답이 들어있지 않은 경우\n","            tf.logging.warning(f\"Could not find answer from context. {answer_from_context} vs {cleaned_answer}\")\n","            continue\n","\n","        else: # inference용\n","          start_pos = -1\n","          end_pos = -1\n","          origin_answer_text = \"\"\n","\n","        example = SquadExample(qas_id=qas_id, question_text=question_text, doc_tokens=doc_tokens,\n","                               origin_answer_text=origin_answer_text, start_pos=start_pos, end_pos=end_pos,\n","                               is_impossible=is_impossible)\n","        examples.append(example)\n","\n","    return examples\n","\n","\n","\n","\n",""],"metadata":{"id":"mIS-ohuH7TiL","executionInfo":{"status":"ok","timestamp":1700038974440,"user_tz":-540,"elapsed":336,"user":{"displayName":"이태후","userId":"03468433647806446322"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["def is_whitespace(c):\n","  if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n","    return True\n","  else:\n","    return False\n","prev_is_whitespace=True\n","doc_tokens = []\n","char_to_word_offset = []\n","ex = \"Psoriasiform dermatitis in a case of newly diagnosed locally advanced pyriform sinus tumour: Bazex syndrome revisited. Acrokeratosis paraneoplastica of Bazex is a rare but important paraneoplastic dermatosis, usually manifesting as psoriasiform rashes over the acral sites. It often precedes diagnosis of the associated malignancy, usually that of upper aerodigestive tract squamous cell carcinoma. We present the case of a patient with a newly diagnosed pyriform sinus tumour and associated acrokeratosis paraneoplastica. To the best of our knowledge, this is the first reported case in the local literature.\"\n","for c in ex:\n","  if is_whitespace(c):\n","    prev_is_whitespace=True\n","  else:\n","    if prev_is_whitespace: # c comes after whitespace -> add as new token\n","      doc_tokens.append(c)\n","    else:\n","      doc_tokens[-1] += c # c comes after c -> attach to recent token\n","    prev_is_whitespace=False\n","  char_to_word_offset.append(len(doc_tokens) -1)"],"metadata":{"id":"CATdoyZz-wl-","executionInfo":{"status":"ok","timestamp":1700038118463,"user_tz":-540,"elapsed":1,"user":{"displayName":"이태후","userId":"03468433647806446322"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["doc_tokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UcdxyFetIEB6","executionInfo":{"status":"ok","timestamp":1700038125351,"user_tz":-540,"elapsed":2,"user":{"displayName":"이태후","userId":"03468433647806446322"}},"outputId":"12029856-0b49-42ae-8070-ea12d56b81f7"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Psoriasiform',\n"," 'dermatitis',\n"," 'in',\n"," 'a',\n"," 'case',\n"," 'of',\n"," 'newly',\n"," 'diagnosed',\n"," 'locally',\n"," 'advanced',\n"," 'pyriform',\n"," 'sinus',\n"," 'tumour:',\n"," 'Bazex',\n"," 'syndrome',\n"," 'revisited.',\n"," 'Acrokeratosis',\n"," 'paraneoplastica',\n"," 'of',\n"," 'Bazex',\n"," 'is',\n"," 'a',\n"," 'rare',\n"," 'but',\n"," 'important',\n"," 'paraneoplastic',\n"," 'dermatosis,',\n"," 'usually',\n"," 'manifesting',\n"," 'as',\n"," 'psoriasiform',\n"," 'rashes',\n"," 'over',\n"," 'the',\n"," 'acral',\n"," 'sites.',\n"," 'It',\n"," 'often',\n"," 'precedes',\n"," 'diagnosis',\n"," 'of',\n"," 'the',\n"," 'associated',\n"," 'malignancy,',\n"," 'usually',\n"," 'that',\n"," 'of',\n"," 'upper',\n"," 'aerodigestive',\n"," 'tract',\n"," 'squamous',\n"," 'cell',\n"," 'carcinoma.',\n"," 'We',\n"," 'present',\n"," 'the',\n"," 'case',\n"," 'of',\n"," 'a',\n"," 'patient',\n"," 'with',\n"," 'a',\n"," 'newly',\n"," 'diagnosed',\n"," 'pyriform',\n"," 'sinus',\n"," 'tumour',\n"," 'and',\n"," 'associated',\n"," 'acrokeratosis',\n"," 'paraneoplastica.',\n"," 'To',\n"," 'the',\n"," 'best',\n"," 'of',\n"," 'our',\n"," 'knowledge,',\n"," 'this',\n"," 'is',\n"," 'the',\n"," 'first',\n"," 'reported',\n"," 'case',\n"," 'in',\n"," 'the',\n"," 'local',\n"," 'literature.']"]},"metadata":{},"execution_count":22}]}]}