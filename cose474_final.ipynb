{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["QB5XanEG-UW2","h21bQ-ZG-REt"],"authorship_tag":"ABX9TyPiM0GwxvVZpf47ZTMWvVIM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### setting"],"metadata":{"id":"QB5XanEG-UW2"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P9ZsF12c9kfj","executionInfo":{"status":"ok","timestamp":1701055091771,"user_tz":-540,"elapsed":2958,"user":{"displayName":"이태후","userId":"03468433647806446322"}},"outputId":"5216b360-71bb-4f18-ed2e-ca86589b9510"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!pip install -q transformers"],"metadata":{"id":"wK2z5YI2kB8u","executionInfo":{"status":"ok","timestamp":1701055103194,"user_tz":-540,"elapsed":11427,"user":{"displayName":"이태후","userId":"03468433647806446322"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### tokenization"],"metadata":{"id":"h21bQ-ZG-REt"}},{"cell_type":"code","source":["# useful functions for data cleansing\n","\n","import unicodedata\n","\n","def decode_bytes(text):\n","\n","  '''check for bytes within text and decode them'''\n","\n","  if isinstance(text, str):\n","    return text\n","  elif isinstance(text, bytes):\n","    return text.decode(\"utf-8\", \"ignore\")\n","  else:\n","    raise ValueError(f\"unsupported string type: {text}\")\n","\n","def whitespace_tokenize(text):\n","\n","  '''strip text and split them'''\n","\n","  text = text.strip()\n","  if not text:\n","    return []\n","  tokens = text.split()\n","  return tokens\n","\n","def is_control(char):\n","\n","  ''' detect control characters except whitespace '''\n","\n","  if unicodedata.category(char).startswith('C'):\n","    if char == '\\t' or char == '\\n' or char == '\\r': # category starts with C but is whitespace\n","      return False\n","    return True\n","  else:\n","    return False\n","\n","def is_whitespace(char):\n","\n","  ''' check whether character is whitespace '''\n","\n","  if char == ' ' or char == '\\t' or char == '\\n' or char == '\\r':\n","    return True\n","  elif unicodedata.category(char) == \"Zs\": # \"space separator\" category\n","    return True\n","  else:\n","    return False\n","\n","def is_punc(char):\n","\n","  ''' check whether character is a punctuation '''\n","  # ~32 : 각종 공백문자, 48~57 : 숫자, 65~90 : 대문자 알파벳, 97~122 : 소문자 알파벳, 126~ : 이상한 기호들\n","  if (ord(char) >= 33 and ord(char) <= 47) or (ord(char) >= 58 and ord(char) <= 64):\n","    return True\n","  elif (ord(char) >= 91 and ord(char) <= 96) or (ord(char) > 123 and ord(char) <= 126):\n","    return True\n","  elif unicodedata.category(char).startswith(\"P\"): # punctuation\n","    return True\n","  else:\n","    return False"],"metadata":{"id":"_-a5ehob-M3v","executionInfo":{"status":"ok","timestamp":1701055103195,"user_tz":-540,"elapsed":17,"user":{"displayName":"이태후","userId":"03468433647806446322"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import collections\n","\n","def load_vocab(vocab_file):\n","\n","  ''' load vocab file as dictionary '''\n","\n","  vocab_dict = collections.OrderedDict()\n","  idx=0\n","\n","  with open(vocab_file, \"r\") as rf:\n","    tokens = rf.read().splitlines()\n","\n","  for token in tokens:\n","    token = decode_bytes(token)\n","    token = token.strip()\n","    vocab_dict[token] = idx\n","    idx += 1\n","\n","  return vocab_dict"],"metadata":{"id":"EABAro7d37Ar","executionInfo":{"status":"ok","timestamp":1701055103195,"user_tz":-540,"elapsed":14,"user":{"displayName":"이태후","userId":"03468433647806446322"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class BasicTokenizer(object):\n","  def __init__(self, do_lower_case=True):\n","    self.do_lower_case = do_lower_case\n","\n","  def clean_text(self, text):\n","\n","    ''' skip invalid characters, convert all whitespaces into single space, and return text '''\n","\n","    output = []\n","    for char in text:\n","      # check if char is NULL,�(unrecognizable), or control character\n","      if ord(char) == 0 or ord(char) == 65533 or is_control(char):\n","        continue\n","      if is_whitespace(char):\n","        output.append(\" \") # all whitespace -> \" \"\n","      else:\n","        output.append(char)\n","    return \"\".join(output)\n","\n","  def run_split_on_punc(self, text):\n","\n","    ''' split text based on punctuations '''\n","\n","    output = []\n","    token = ''\n","    for char in list(text):\n","      if is_punc(char):\n","        output.append(token)\n","        output.append(char)\n","        token = ''\n","      else:\n","        token += char\n","\n","    if len(token) > 0:\n","      output.append(token)\n","\n","    return output\n","\n","  def tokenize(self, text):\n","    text = decode_bytes(text)\n","    text = self.clean_text(text)\n","\n","    orig_tokens = whitespace_tokenize(text) # 공백문자 단위로 분리\n","    split_tokens = []\n","\n","    for token in orig_tokens: # 토큰별로 punctuation 있을 경우 분리\n","      if self.do_lower_case:\n","        token = token.lower()\n","      split_tokens.extend(self.run_split_on_punc(token))\n","\n","    return split_tokens"],"metadata":{"id":"qinZXvskH4a6","executionInfo":{"status":"ok","timestamp":1701055937510,"user_tz":-540,"elapsed":3,"user":{"displayName":"이태후","userId":"03468433647806446322"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["class WordpieceTokenizer(object):\n","  def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n","    self.vocab = vocab\n","    self.unk_token = unk_token\n","    self.max_input_chars_per_word = max_input_chars_per_word\n","\n","  def tokenize(self, text):\n","    ''' input : a single token (via BasicTokenizer), output : wordpiece tokens\n","\n","    wordpiece merges characters by \"score = freq(pair) / freq(first)*freq(second)\".\n","    pairs that frequently appear together are merged, but not if each element also appears frequently\n","\n","    ex) unable : \"un\", \"##able\"\n","    ex) hugging : \"hugging\" '''\n","\n","    text = decode_bytes(text)\n","\n","    output_tokens = []\n","    for token in whitespace_tokenize(text):\n","      chars = list(token)\n","      # 너무 긴 단어 (>200)는 unk 처리\n","      if len(chars) > self.max_input_chars_per_word:\n","        output_tokens.append(self.unk_token)\n","        continue\n","\n","      is_bad = False\n","      start = 0\n","      sub_tokens = []\n","\n","      # ex. unfriendly : length 10\n","      while start < len(chars): # 0~9\n","        end = len(chars) # 10\n","        cur_substr = None\n","        while start < end:\n","          substr = \"\".join(chars[start:end]) # unfriendly, unfriendl, unfriend, ... , un\n","          if start > 0:\n","            substr = \"##\" + substr # indicates subword\n","          if substr in self.vocab: # \"unfriendly\" not in vocab / ... \"un\" in vocab\n","            cur_substr = substr\n","            break\n","          end -= 1\n","        if cur_substr is None:\n","          is_bad = True\n","          break\n","        sub_tokens.append(cur_substr)\n","\n","        start = end # subword 떨어져나간 경우 (un) 끝자리->시작자리 (f)\n","\n","      if is_bad: # character 한개도 vocab에 없는 경우\n","        output_tokens.append(self.unk_token)\n","      else:\n","        output_tokens.extend(sub_tokens)\n","\n","    return output_tokens\n","\n"],"metadata":{"id":"1kgEsgaQnrCa","executionInfo":{"status":"ok","timestamp":1701055103195,"user_tz":-540,"elapsed":13,"user":{"displayName":"이태후","userId":"03468433647806446322"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class FullTokenizer(object):\n","  def __init__(self, vocab_file, do_lower_case=True):\n","    self.vocab = load_vocab(vocab_file)\n","    self.vocab_reverse = {v:k for k,v in self.vocab.items()}\n","    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n","    self.wordpiece = WordpieceTokenizer(vocab=self.vocab)\n","\n","  def tokenize(self, text):\n","    split_tokens = []\n","    for token in self.basic_tokenizer.tokenize(text): # cleanse text, tokenize by punctuation\n","      for sub_token in self.wordpiece.tokenize(token): # tokenize tokens (subtokens)\n","        split_tokens.append(sub_token)\n","\n","    return split_tokens\n","\n","  def convert_tokens_to_ids(self, tokens):\n","    output = []\n","    for token in tokens:\n","      output.append(self.vocab[token])\n","    return output"],"metadata":{"id":"E9y_UXkh3yQC","executionInfo":{"status":"ok","timestamp":1701071711020,"user_tz":-540,"elapsed":498,"user":{"displayName":"이태후","userId":"03468433647806446322"}}},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":["### finetune biobert"],"metadata":{"id":"BJBzS902_U93"}},{"cell_type":"code","source":["from absl import flags\n","\n","flags.DEFINE_string(\"bert_config_file\", None, \"config json file corresponding to the pretrained BERT model\")\n","flags.DEFINE_string(\"vocab_File\", None, \"vocab file on which BERT was trained\")\n","flags.DEFINE_string(\"output_dir\", None, \"output dir for model checkpoints\")\n","flags.DEFINE_string(\"train_file\", None, \"squad-formatted json for training. E.g., train-v1.1.json\")\n","flags.DEFINE_string(\"predict_file\", None, \"squad-formatted json for predictions. E.g., dev(test)-v1.1.json\")\n","flags.DEFINE_string(\"init_checkpoint\", None, \"initial checkpoint (usually from a pretrained BERT model)\")\n","\n","flags.DEFINE_bool(\"do_lower_case\", True, \"whether to lower-case input text. True for lower case.\")\n","flags.DEFINE_bool(\"do_train\", True, \"whether to run training\")\n","flags.DEFINE_bool(\"do_predict\", True, \"whether to run eval on the dev set\")\n","flags.DEFINE_bool(\"use_tpu\", False, \"whether to use TPU or GPU/CPU\")\n","flags.DEFINE_bool(\"verbose_logging\", False, \"whether to print all warnings during data processing. Other warnings are printed by default.\")\n","\n","flags.DEFINE_integer(\"max_seq_len\", 384, \"maximum input sequence after WordPiece tokenization. Will be padded if shorter, truncated if longer.\")\n","flags.DEFINE_integer(\"doc_stride\", 128, \"when splitting up a long document into chunks, how much stride to take between chunks\")\n","flags.DEFINE_integer(\"max_query_len\", 64, \"maximum query sequence after WordPiece tokenization. Will be truncated if longer.\")\n","flags.DEFINE_integer(\"predict_batch_size\", 8, \"total batch size for predictions\")\n","flags.DEFINE_integer(\"save_checkpoint_step\", 1000, \"how often to save model checkpoints\")\n","flags.DEFINE_integer(\"iterations_per_loop\", 1000, \"how many steps to make in each estimator call\")\n","flags.DEFINE_integer(\"n_best_size\", 20, \"how many n-best predictions to generate in nbest_predictions.json output file\")\n","flags.DEFINE_integer(\"max_answer_len\", 30, \"maximum length of generated answer\")\n","\n","flags.DEFINE_float(\"learning_rate\", 5e-5, \"initial learning rate for Adam optimizer\")\n","flags.DEFINE_float(\"num_train_epochs\", 3.0, \"number of training epochs\")\n","flags.DEFINE_float(\"warmup_proportion\", 0.1, \"proportion of training for linear lr warmup. E.g., 0.1 refers to 10% of training\")\n","flags.DEFINE_float(\"null_score_diff_threshold\", 0.0, \"to predict null if (null_score - best_non_null) > threshold\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M6iOQ_ARlZpZ","executionInfo":{"status":"ok","timestamp":1701055103195,"user_tz":-540,"elapsed":12,"user":{"displayName":"이태후","userId":"03468433647806446322"}},"outputId":"946eb988-5b72-4561-9502-b1f0ca8b1e0f"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<absl.flags._flagvalues.FlagHolder at 0x7fae0c1a1b40>"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["class SquadExample(object):\n","  def __init__(self, qas_id, question_text, doc_tokens, origin_answer_text=None, start_pos=None, end_pos=None):\n","    self.qas_id = qas_id\n","    self.question_text = question_text\n","    self.doc_tokens = doc_tokens\n","    self.origin_answer_text = origin_answer_text\n","    self.start_pos = start_pos\n","    self.end_pos = end_pos\n","\n","  def __str__(self):\n","    return self.__repr__()\n","\n","  def __repr__(self):\n","    s = f\"qas_id: {decode_bytes(self.qas_id)}\" # check if text is in str, bytes or unicode and convert\n","    s += f\", question_text: {decode_bytes(self.question_text)}\"\n","    s += f\", doc_tokens: {[' '.join(self.doc_tokens)]}\"\n","\n","    if self.start_pos:\n","      s += f\", start_pos: {self.start_pos}, end_pos: {self.end_pos}\"\n","\n","    return s\n"],"metadata":{"id":"CAu4TqjuvWVJ","executionInfo":{"status":"ok","timestamp":1701055103651,"user_tz":-540,"elapsed":465,"user":{"displayName":"이태후","userId":"03468433647806446322"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def read_squad_examples(input_file, is_training):\n","  \"\"\"Read a squad json file into a list of SquadExample\"\"\"\n","\n","  with open(input_file, \"r\") as rf:\n","    input_data = json.load(rf)[\"data\"][0][\"paragraphs\"]\n","\n","  examples = []\n","  for entry in input_data:\n","    doc_tokens = []\n","    char_to_word_offset = []\n","    prev_is_whitespace = True\n","\n","    context = entry[\"context\"]\n","    for c in context:\n","      if is_whitespace(c):\n","        prev_is_whitespace=True\n","      else:\n","        if prev_is_whitespace: # c comes after whitespace -> add as new token\n","          doc_tokens.append(c)\n","        else:\n","          doc_tokens[-1] += c # c comes after c -> attach to recent token\n","        prev_is_whitespace=False\n","      char_to_word_offset.append(len(doc_tokens) -1) # \"I went home\" -> [0,0,1,1,1,1,1,2,2,2,2]\n","\n","    qas = entry[\"qas\"]\n","    for qa in qas:\n","      qas_id = qa[\"id\"]\n","      question_text = qa[\"question\"]\n","      start_pos = None\n","      end_pos = None\n","      origin_answer_text = None\n","\n","      if is_training:\n","        if len(qa[\"answers\"]) != 1:\n","          raise ValueError(\"Each question should have exactly 1 answer.\")\n","\n","        answer = qa[\"answers\"][0]\n","        origin_answer_text = answer[\"text\"] #\"Bazex syndrome\"\n","        answer_offset = answer[\"answer_start\"] #93\n","        answer_length = len(origin_answer_text)\n","        start_pos = char_to_word_offset[answer_offset] # 몇번째 단어부터\n","        end_pos = char_to_word_offset[answer_offset + answer_length-1] # 몇번째 단어까지\n","\n","        # check if answer text is extractable from context (아닌 경우 건너뜀)\n","        answer_from_context = \" \".join(doc_tokens[start_pos:end_pos+1]) # context에서 주어진 범위로 인덱싱한 정답\n","        cleaned_answer = \" \".join(whitespace_tokenize(origin_answer_text)) # 실제 정답 (strip 및 split)\n","\n","        if answer_from_context.find(cleaned_answer) == -1: # context에 정답이 들어있지 않은 경우\n","          tf.logging.warning(f\"Could not find answer from context. {answer_from_context} vs {cleaned_answer}\")\n","          continue\n","\n","      else: # inference용\n","        start_pos = -1\n","        end_pos = -1\n","        origin_answer_text = \"\"\n","\n","      example = SquadExample(qas_id=qas_id, question_text=question_text, doc_tokens=doc_tokens,\n","                              origin_answer_text=origin_answer_text, start_pos=start_pos, end_pos=end_pos)\n","      examples.append(example)\n","\n","  return examples"],"metadata":{"id":"Jb-H6ed4BC7Q","executionInfo":{"status":"ok","timestamp":1701055109078,"user_tz":-540,"elapsed":2,"user":{"displayName":"이태후","userId":"03468433647806446322"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["examples = read_squad_examples('/content/drive/MyDrive/cose474_final/trainset_bioasq.json', is_training=True)"],"metadata":{"id":"N2Pa84IpUCGT","executionInfo":{"status":"ok","timestamp":1701055128628,"user_tz":-540,"elapsed":19552,"user":{"displayName":"이태후","userId":"03468433647806446322"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["len(examples)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tI5bmUTNwD6E","executionInfo":{"status":"ok","timestamp":1701055128629,"user_tz":-540,"elapsed":20,"user":{"displayName":"이태후","userId":"03468433647806446322"}},"outputId":"c980f5c6-bf1a-4dd5-fff0-101006db2bf2"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["11171"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["def check_is_max_context(doc_spans, cur_span_idx, pos):\n","  '''check whether a span gives maximum \"score\" for a token.\n","  ex) Span A: the man went to the\n","  Span B: to the store and bought\n","  Span C: and bought a gallon of\n","\n","  \"bought\" will have 2 scores from span B and C.\n","  we choose span with maximum context, which is defined as min(left, right).\n","  Span B : min(4,0) = 0\n","  Span C : min(1,3) = 1\n","\n","  We choose Span C as the maximum context for the token \"bought\".'''\n","\n","  best_score = None\n","  best_span_idx = None\n","\n","  for idx, doc_span in enumerate(doc_spans):\n","    end = doc_span.start + doc_span.length - 1\n","    if pos < doc_span.start:\n","      continue\n","    if pos > end:\n","      continue\n","    num_left_context = pos - doc_span.start\n","    num_right_context = end - pos\n","    score = min(num_left_context, num_right_context) + 0.01*doc_span.length\n","    if best_score is None or score > best_score:\n","      best_score = score\n","      best_span_idx = idx\n","\n","  return cur_span_idx == best_span_idx"],"metadata":{"id":"4bb0IhsYpDNr","executionInfo":{"status":"ok","timestamp":1701070209502,"user_tz":-540,"elapsed":3,"user":{"displayName":"이태후","userId":"03468433647806446322"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["import json\n","import tensorflow as tf\n","\n","class InputFeatures(object):\n","  def __init__(self, unique_id, example_index, doc_span_index, tokens, token_to_origin_map, token_is_max_context,\n","             input_ids, input_mask, segment_ids, start_pos=None, end_pos=None):\n","    self.unique_id = unique_id\n","    self.example_index = example_index\n","    self.doc_span_index = doc_span_index\n","    self.tokens = tokens\n","    self.token_to_origin_map = token_to_origin_map\n","    self.token_is_max_context = token_is_max_context\n","    self.input_ids = input_ids\n","    self.input_mask = input_mask\n","    self.segment_ids = segment_ids\n","    self.start_pos = start_pos\n","    self.end_pos = end_pos"],"metadata":{"id":"mIS-ohuH7TiL","executionInfo":{"status":"ok","timestamp":1701075550937,"user_tz":-540,"elapsed":448,"user":{"displayName":"이태후","userId":"03468433647806446322"}}},"execution_count":88,"outputs":[]},{"cell_type":"code","source":["def convert_examples_to_features(examples, tokenizer, max_seq_len, doc_stride, max_query_len, is_training, output_fn):\n","  unique_id = 1000000000\n","\n","  for example_idx, example in enumerate(examples):\n","    query_tokens = tokenizer.tokenize(example.question_text)\n","\n","    if len(query_tokens) > max_query_len:\n","      query_tokens = query_tokens[:max_query_len] # query 최대 길이로 제한\n","\n","    word_to_subtoken_idx = [] # 각 단어가 subtoken 기준으로 몇번째인지 (ex. 3번째 단어 -> 9번째 subword부터)\n","    subtoken_to_word_idx = [] # 각 subtoken이 원래 몇번째 단어에 속하는지 (ex. 9번째 subword -> 3번째 단어)\n","    all_doc_tokens = [] # 모든 subtoken 모음\n","\n","    for i, token in enumerate(example.doc_tokens): # Psoriasiform, dermatitis, in, a, case, ...\n","      word_to_subtoken_idx.append(len(all_doc_tokens))\n","      sub_tokens = tokenizer.tokenize(token) # tokenize into subwords\n","      for sub_token in sub_tokens:\n","        subtoken_to_word_idx.append(i)\n","        all_doc_tokens.append(sub_token)\n","\n","    tok_start_pos = None\n","    tok_end_pos = None\n","\n","    print(f'all_doc_tokens : {all_doc_tokens}')\n","    print(f'word_to_subtoken_idx : {word_to_subtoken_idx}')\n","    print(f'subtoken_to_word_idx : {subtoken_to_word_idx}')\n","\n","    if is_training:\n","      tok_start_pos = word_to_subtoken_idx[example.start_pos]\n","      if example.end_pos < len(example.doc_tokens) - 1:\n","        tok_end_pos = word_to_subtoken_idx[example.end_pos + 1] - 1\n","      else: # 범위 벗어난 경우\n","        tok_end_pos = len(all_doc_tokens) - 1\n","\n","    print(f'tok_start_pos : {tok_start_pos}')\n","    print(f'tok_end_pos : {tok_end_pos}')\n","    print(f'answer tokens : {all_doc_tokens[tok_start_pos:tok_end_pos+1]}')\n","\n","    # max_seq_len = len(query_tokens) + len(doc_tokens) + 3 ([CLS], [SEP], [SEP] : 문장 시작, 문장 구분, 문장 끝)\n","    max_tokens_for_doc = max_seq_len - len(query_tokens) - 3 # (384-11-3) = 370\n","\n","    # sliding window for long document training\n","    Docspan = collections.namedtuple('Docspan', ['start', 'length'])\n","    doc_spans = []\n","    start = 0 # 0 -> 128 -> 256 -> 384\n","    while start < len(all_doc_tokens):\n","      length = len(all_doc_tokens) - start # 729 -> 601 -> 473 -> 345\n","      if length > max_tokens_for_doc:\n","        length = max_tokens_for_doc\n","      doc_spans.append(Docspan(start=start, length=length))\n","      if start + length == len(all_doc_tokens): # 384 + 345 = 729\n","        break\n","      start += min(length, doc_stride)\n","    print(doc_spans)\n","\n","    for doc_span_idx, doc_span in enumerate(doc_spans):\n","      tokens = []\n","      token_to_orig_map = {}\n","      token_is_max_context = {}\n","      segment_ids = [] # [cls]query[sep] / doc[sep] : 00000/11111 로 구분\n","\n","      tokens.append(\"[CLS]\")\n","      segment_ids.append(0)\n","\n","      for token in query_tokens:\n","        tokens.append(token)\n","        segment_ids.append(0)\n","      tokens.append(\"[SEP]\")\n","      segment_ids.append(0)\n","\n","      for i in range(doc_span.length):\n","        doc_idx = doc_span.start + i\n","        token_to_orig_map[len(tokens)] = subtoken_to_word_idx[doc_idx]\n","\n","        is_max_context = check_is_max_context(doc_spans, doc_span_idx, doc_idx)\n","        token_is_max_context[len(tokens)] = is_max_context\n","        tokens.append(all_doc_tokens[doc_idx])\n","        segment_ids.append(1)\n","\n","      tokens.append(\"[SEP]\")\n","      segment_ids.append(1)\n","\n","      # encode tokens to ids (index numbers of vocabs)\n","      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","      # masking (1 for real tokens, 0 for padding tokens)\n","      input_mask = [1] * len(input_ids)\n","      while len(input_ids) < max_seq_len:\n","        input_ids.append(0)\n","        input_mask.append(0)\n","        segment_ids.append(0)\n","\n","      assert len(input_ids) == max_seq_len\n","      assert len(input_mask) == max_seq_len\n","      assert len(segment_ids) == max_seq_len\n","\n","      start_pos = None\n","      end_pos = None\n","\n","      if is_training:\n","        doc_start = doc_span.start\n","        doc_end = doc_span.start + doc_span.length - 1\n","        out_of_span = False\n","\n","        if not (tok_start_pos >= doc_start and tok_end_pos <= doc_end): # tok_start_pos : word_to_subtoken_idx[example.start_pos]\n","          out_of_span = True\n","        if out_of_span:\n","          start_pos = 0\n","          end_pos = 0\n","        else:\n","          doc_offset = len(query_tokens) + 2\n","          start_pos = tok_start_pos - doc_start + doc_offset\n","          end_pos = tok_end_pos - doc_start + doc_offset\n","\n","      ##############################################################################################################\n","\n","      feature = InputFeatures(\n","          unique_id=unique_id,\n","          example_index=example_idx,\n","          doc_span_index=doc_span_idx,\n","          tokens=tokens,\n","          token_to_origin_map=token_to_orig_map,\n","          token_is_max_context=token_is_max_context,\n","          input_ids=input_ids,\n","          input_mask=input_mask,\n","          segment_ids=segment_ids,\n","          start_pos=start_pos,\n","          end_pos=end_pos\n","      )\n","      output_fn(feature)\n","\n","      unique_id += 1\n"],"metadata":{"id":"fOOxoauhVffT","executionInfo":{"status":"ok","timestamp":1701075920940,"user_tz":-540,"elapsed":545,"user":{"displayName":"이태후","userId":"03468433647806446322"}}},"execution_count":93,"outputs":[]},{"cell_type":"code","source":["# 10번째~12번째 단어가 정답 범위. subtokenize 이후에는 subtoken 단위로 변경해줘야.\n","examples[17]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P35omlORAlS_","executionInfo":{"status":"ok","timestamp":1701066753122,"user_tz":-540,"elapsed":3,"user":{"displayName":"이태후","userId":"03468433647806446322"}},"outputId":"6f95a137-5c19-4185-e373-398330ab9242"},"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["qas_id: 56c1f01def6e394741000045_001, question_text: Orteronel was developed for treatment of which cancer?, doc_tokens: ['Efficacy and safety of second-line agents for treatment of metastatic castration-resistant prostate cancer progressing after docetaxel. A systematic review and meta-analysis. We performed a systematic review of the literature to assess the efficacy and the safety of second-line agents targeting metastatic castration-resistant prostate cancer (mCRPC) that has progressed after docetaxel. Pooled-analysis was also performed, to assess the effectiveness of agents targeting the androgen axis via identical mechanisms of action (abiraterone acetate, orteronel). MATERIALS AND We included phase III randomized controlled trials that enrolled patients with mCRPC progressing during or after first-line docetaxel treatment. Trials were identified by electronic database searching. The primary outcome of the review was overall survival. Secondary outcomes were radiographic progression-free survival (rPFS) and severe adverse effects (grade 3 or higher). Ten articles met the inclusion criteria for the review. These articles reported the results of five clinical trials, enrolling in total 5047 patients. The experimental interventions tested in these studies were enzalutamide, ipilimumab, abiraterone acetate, orteronel and cabazitaxel. Compared to control cohorts (active drug-treated or placebo-treated), the significant overall survival advantages achieved were 4.8 months for enzalutamide (hazard ratio for death vs. placebo: 0.63; 95% CI 0.53 to 0.75, P < 0.0001), 4.6 months for abiraterone (hazard ratio for death vs. placebo: 0.66, 95% CI 0.58 to 0.75, P < 0.0001) and 2.4 months for cabazitaxel (hazard ratio for death vs. mitoxantrone-prednisone: 0.70, 95% CI 0.59 to 0.83, p < 0.0001). Pooled analysis of androgen synthesis inhibitors orteronel and abiraterone resulted in significantly increased overall and progression-free survival for anti-androgen agents, compared to placebo (hazard ratio for death: 0.76, 95% CI 0.67 to 0.87, P < 0.0001; hazard ratio for radiographic progression: 0.7, 95% CI 0.63 to 0.77, P < 0.00001). Androgen synthesis inhibitors induced significant increases in risk ratios for adverse effects linked to elevated mineralocorticoid secretion, compared to placebo (risk ratio for hypokalemia: 5.75, 95% CI 2.08 to 15.90; P = 0.0008; risk-ratio for hypertension: 2.29, 95% CI 1.02 to 5.17; P = 0.05). In docetaxel-pretreated patients enzalutamide, abiraterone-prednisone and cabazitaxel-prednisone can improve overall survival of patients, compared to placebo or to best of care at the time of study (mitoxantrone-prednisone). Agents targeting the androgen axis (enzalutamide, abiraterone, orteronel) significantly prolonged rPFS, compared to placebo. Further investigation is warranted to evaluate the benefit of combination or sequential administration of these agents. Large-scale studies are also necessary to evaluate the impact of relevant toxic effects observed in a limited number of patients (e.g., enzalutamide-induced seizures, orteronel-induced pancreatitis, and others).'], start_pos: 10, end_pos: 12"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":["# convert_examples_to_features([examples[17]], FullTokenizer('/content/drive/MyDrive/cose474_final/vocab_biobert_large_cased.txt', do_lower_case=False), 384, 128, 64, True, None)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":474},"id":"avRLDCA5zbQR","executionInfo":{"status":"error","timestamp":1701076082851,"user_tz":-540,"elapsed":336,"user":{"displayName":"이태후","userId":"03468433647806446322"}},"outputId":"6bf77664-be75-4a5d-d9f6-28faf942bab6"},"execution_count":95,"outputs":[{"output_type":"stream","name":"stdout","text":["all_doc_tokens : ['Eff', '##ica', '##cy', 'and', 'safety', 'of', 'second', '-', 'line', 'agents', 'for', 'treatment', 'of', 'meta', '##static', 'cast', '##ration', '-', 'resistant', 'prost', '##ate', 'cancer', 'progress', '##ing', 'after', 'do', '##ce', '##tax', '##el', '.', 'A', 'systematic', 'review', 'and', 'meta', '-', 'analysis', '.', 'We', 'performed', 'a', 'systematic', 'review', 'of', 'the', 'literature', 'to', 'assess', 'the', 'efficacy', 'and', 'the', 'safety', 'of', 'second', '-', 'line', 'agents', 'targeting', 'meta', '##static', 'cast', '##ration', '-', 'resistant', 'prost', '##ate', 'cancer', '(', 'm', '##CRP', '##C', ')', 'that', 'has', 'progressed', 'after', 'do', '##ce', '##tax', '##el', '.', 'Poole', '##d', '-', 'analysis', 'was', 'also', 'performed', ',', 'to', 'assess', 'the', 'effectiveness', 'of', 'agents', 'targeting', 'the', 'androgen', 'axis', 'via', 'identical', 'mechanisms', 'of', 'action', '(', 'abi', '##rate', '##rone', 'acetate', ',', 'ort', '##eron', '##el', ')', '.', 'MA', '##TE', '##RIA', '##LS', 'AND', 'We', 'included', 'phase', 'III', 'random', '##ized', 'controlled', 'trials', 'that', 'enrolled', 'patients', 'with', 'm', '##CRP', '##C', 'progress', '##ing', 'during', 'or', 'after', 'first', '-', 'line', 'do', '##ce', '##tax', '##el', 'treatment', '.', 'Trials', 'were', 'identified', 'by', 'electronic', 'database', 'searching', '.', 'The', 'primary', 'outcome', 'of', 'the', 'review', 'was', 'overall', 'survival', '.', 'Secondary', 'outcomes', 'were', 'radio', '##graphic', 'progression', '-', 'free', 'survival', '(', 'r', '##PFS', ')', 'and', 'severe', 'adverse', 'effects', '(', 'grade', '3', 'or', 'higher', ')', '.', 'Ten', 'articles', 'met', 'the', 'inclusion', 'criteria', 'for', 'the', 'review', '.', 'These', 'articles', 'reported', 'the', 'results', 'of', 'five', 'clinical', 'trials', ',', 'en', '##roll', '##ing', 'in', 'total', '504', '##7', 'patients', '.', 'The', 'experimental', 'interventions', 'tested', 'in', 'these', 'studies', 'were', 'enza', '##lut', '##amide', ',', 'ip', '##ili', '##mum', '##ab', ',', 'abi', '##rate', '##rone', 'acetate', ',', 'ort', '##eron', '##el', 'and', 'cab', '##azi', '##tax', '##el', '.', 'Compared', 'to', 'control', 'coh', '##ort', '##s', '(', 'active', 'drug', '-', 'treated', 'or', 'placebo', '-', 'treated', ')', ',', 'the', 'significant', 'overall', 'survival', 'advantages', 'achieved', 'were', '4', '.', '8', 'months', 'for', 'enza', '##lut', '##amide', '(', 'hazard', 'ratio', 'for', 'death', 'vs', '.', 'placebo', ':', '0', '.', '63', ';', '95', '%', 'CI', '0', '.', '53', 'to', '0', '.', '75', ',', 'P', '<', '0', '.', '0001', ')', ',', '4', '.', '6', 'months', 'for', 'abi', '##rate', '##rone', '(', 'hazard', 'ratio', 'for', 'death', 'vs', '.', 'placebo', ':', '0', '.', '66', ',', '95', '%', 'CI', '0', '.', '58', 'to', '0', '.', '75', ',', 'P', '<', '0', '.', '0001', ')', 'and', '2', '.', '4', 'months', 'for', 'cab', '##azi', '##tax', '##el', '(', 'hazard', 'ratio', 'for', 'death', 'vs', '.', 'mit', '##ox', '##ant', '##rone', '-', 'prednis', '##one', ':', '0', '.', '70', ',', '95', '%', 'CI', '0', '.', '59', 'to', '0', '.', '83', ',', 'p', '<', '0', '.', '0001', ')', '.', 'Poole', '##d', 'analysis', 'of', 'androgen', 'synthesis', 'inhibitor', '##s', 'ort', '##eron', '##el', 'and', 'abi', '##rate', '##rone', 'resulted', 'in', 'significantly', 'increased', 'overall', 'and', 'progression', '-', 'free', 'survival', 'for', 'anti', '-', 'androgen', 'agents', ',', 'compared', 'to', 'placebo', '(', 'hazard', 'ratio', 'for', 'death', ':', '0', '.', '76', ',', '95', '%', 'CI', '0', '.', '67', 'to', '0', '.', '87', ',', 'P', '<', '0', '.', '0001', ';', 'hazard', 'ratio', 'for', 'radio', '##graphic', 'progression', ':', '0', '.', '7', ',', '95', '%', 'CI', '0', '.', '63', 'to', '0', '.', '77', ',', 'P', '<', '0', '.', '000', '##01', ')', '.', 'And', '##rogen', 'synthesis', 'inhibitor', '##s', 'induced', 'significant', 'increases', 'in', 'risk', 'ratios', 'for', 'adverse', 'effects', 'linked', 'to', 'elevated', 'mineral', '##oc', '##ort', '##ico', '##id', 'secret', '##ion', ',', 'compared', 'to', 'placebo', '(', 'risk', 'ratio', 'for', 'hyp', '##oka', '##lem', '##ia', ':', '5', '.', '75', ',', '95', '%', 'CI', '2', '.', '08', 'to', '15', '.', '90', ';', 'P', '=', '0', '.', '000', '##8', ';', 'risk', '-', 'ratio', 'for', 'hypertension', ':', '2', '.', '29', ',', '95', '%', 'CI', '1', '.', '02', 'to', '5', '.', '17', ';', 'P', '=', '0', '.', '05', ')', '.', 'In', 'do', '##ce', '##tax', '##el', '-', 'pretreated', 'patients', 'enza', '##lut', '##amide', ',', 'abi', '##rate', '##rone', '-', 'prednis', '##one', 'and', 'cab', '##azi', '##tax', '##el', '-', 'prednis', '##one', 'can', 'improve', 'overall', 'survival', 'of', 'patients', ',', 'compared', 'to', 'placebo', 'or', 'to', 'best', 'of', 'care', 'at', 'the', 'time', 'of', 'study', '(', 'mit', '##ox', '##ant', '##rone', '-', 'prednis', '##one', ')', '.', 'Agents', 'targeting', 'the', 'androgen', 'axis', '(', 'enza', '##lut', '##amide', ',', 'abi', '##rate', '##rone', ',', 'ort', '##eron', '##el', ')', 'significantly', 'prolonged', 'r', '##PFS', ',', 'compared', 'to', 'placebo', '.', 'Further', 'investigation', 'is', 'warrant', '##ed', 'to', 'evaluate', 'the', 'benefit', 'of', 'combination', 'or', 'sequent', '##ial', 'administration', 'of', 'these', 'agents', '.', 'Large', '-', 'scale', 'studies', 'are', 'also', 'necessary', 'to', 'evaluate', 'the', 'impact', 'of', 'relevant', 'toxic', 'effects', 'observed', 'in', 'a', 'limited', 'number', 'of', 'patients', '(', 'e', '.', 'g', '.', ',', 'enza', '##lut', '##amide', '-', 'induced', 'seizure', '##s', ',', 'ort', '##eron', '##el', '-', 'induced', 'pan', '##creat', '##itis', ',', 'and', 'others', ')', '.']\n","word_to_subtoken_idx : [0, 3, 4, 5, 6, 9, 10, 11, 12, 13, 15, 19, 21, 22, 24, 25, 30, 31, 32, 33, 34, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 57, 58, 59, 61, 65, 67, 68, 73, 74, 75, 76, 77, 82, 86, 87, 88, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 109, 111, 116, 120, 121, 122, 123, 124, 125, 127, 128, 129, 130, 131, 132, 133, 136, 138, 139, 140, 141, 144, 148, 150, 151, 152, 153, 154, 155, 156, 158, 159, 160, 161, 162, 163, 164, 165, 166, 168, 169, 170, 171, 173, 176, 177, 181, 182, 183, 184, 185, 187, 188, 189, 192, 193, 194, 195, 196, 197, 198, 199, 200, 202, 203, 204, 205, 206, 207, 208, 209, 210, 212, 215, 216, 217, 219, 221, 222, 223, 224, 225, 226, 227, 228, 229, 233, 238, 241, 243, 246, 247, 252, 253, 254, 255, 258, 260, 263, 264, 269, 270, 271, 272, 273, 274, 275, 276, 279, 280, 281, 284, 286, 287, 288, 289, 291, 293, 297, 299, 300, 303, 304, 308, 309, 310, 315, 318, 319, 320, 323, 325, 326, 327, 328, 330, 332, 336, 338, 339, 342, 343, 347, 348, 349, 353, 354, 357, 358, 359, 363, 365, 366, 367, 368, 370, 378, 382, 384, 385, 388, 389, 393, 394, 395, 400, 402, 403, 404, 405, 406, 408, 411, 412, 415, 416, 417, 418, 419, 420, 421, 424, 425, 426, 429, 431, 432, 433, 434, 436, 437, 438, 440, 444, 446, 447, 450, 451, 455, 456, 457, 461, 462, 463, 464, 466, 468, 472, 474, 475, 478, 479, 483, 484, 485, 491, 493, 494, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 513, 516, 517, 518, 519, 521, 522, 523, 528, 532, 534, 535, 538, 539, 543, 544, 545, 550, 553, 554, 556, 560, 562, 563, 566, 567, 571, 572, 573, 578, 579, 585, 586, 590, 596, 597, 604, 605, 606, 607, 608, 609, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 634, 635, 636, 637, 638, 639, 644, 648, 652, 653, 654, 657, 658, 659, 661, 662, 663, 664, 666, 667, 668, 669, 670, 671, 672, 673, 675, 676, 677, 678, 680, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 708, 713, 716, 721, 725, 726]\n","subtoken_to_word_idx : [0, 0, 0, 1, 2, 3, 4, 4, 4, 5, 6, 7, 8, 9, 9, 10, 10, 10, 10, 11, 11, 12, 13, 13, 14, 15, 15, 15, 15, 15, 16, 17, 18, 19, 20, 20, 20, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 37, 37, 38, 39, 40, 40, 41, 41, 41, 41, 42, 42, 43, 44, 44, 44, 44, 44, 45, 46, 47, 48, 49, 49, 49, 49, 49, 50, 50, 50, 50, 51, 52, 53, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 69, 69, 69, 70, 70, 71, 71, 71, 71, 71, 72, 72, 72, 72, 73, 74, 75, 76, 77, 78, 78, 79, 80, 81, 82, 83, 84, 85, 85, 85, 86, 86, 87, 88, 89, 90, 90, 90, 91, 91, 91, 91, 92, 92, 93, 94, 95, 96, 97, 98, 99, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 108, 109, 110, 111, 112, 112, 113, 113, 113, 114, 115, 115, 115, 115, 116, 117, 118, 119, 120, 120, 121, 122, 123, 123, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 141, 142, 142, 142, 143, 144, 145, 145, 146, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 155, 155, 155, 156, 156, 156, 156, 156, 157, 157, 157, 158, 158, 159, 159, 159, 160, 161, 161, 161, 161, 161, 162, 163, 164, 165, 165, 165, 166, 166, 167, 167, 167, 168, 169, 169, 169, 169, 169, 170, 171, 172, 173, 174, 175, 176, 177, 177, 177, 178, 179, 180, 180, 180, 181, 181, 182, 183, 184, 185, 185, 186, 186, 187, 187, 187, 187, 188, 188, 189, 190, 190, 190, 191, 192, 192, 192, 192, 193, 194, 195, 195, 195, 195, 195, 196, 196, 196, 197, 198, 199, 199, 199, 200, 200, 201, 202, 203, 204, 204, 205, 205, 206, 206, 206, 206, 207, 207, 208, 209, 209, 209, 210, 211, 211, 211, 211, 212, 213, 214, 214, 214, 214, 215, 216, 216, 216, 217, 218, 219, 219, 219, 219, 220, 220, 221, 222, 223, 224, 224, 225, 225, 225, 225, 225, 225, 225, 225, 226, 226, 226, 226, 227, 227, 228, 229, 229, 229, 230, 231, 231, 231, 231, 232, 233, 234, 234, 234, 234, 234, 235, 235, 236, 237, 238, 239, 240, 240, 241, 241, 241, 242, 243, 243, 243, 244, 245, 246, 247, 248, 249, 250, 250, 250, 251, 252, 253, 253, 253, 254, 254, 255, 256, 257, 258, 258, 259, 260, 261, 261, 262, 262, 262, 262, 263, 263, 264, 265, 265, 265, 266, 267, 267, 267, 267, 268, 269, 270, 270, 270, 270, 271, 272, 273, 274, 274, 275, 275, 276, 276, 276, 276, 277, 277, 278, 279, 279, 279, 280, 281, 281, 281, 281, 282, 283, 284, 284, 284, 284, 284, 284, 285, 285, 286, 287, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 300, 300, 300, 300, 301, 301, 301, 302, 303, 304, 305, 305, 306, 307, 308, 308, 308, 308, 308, 309, 309, 309, 309, 310, 310, 311, 312, 312, 312, 313, 314, 314, 314, 314, 315, 316, 317, 317, 317, 317, 317, 318, 318, 318, 319, 320, 320, 321, 321, 321, 321, 322, 322, 323, 324, 324, 324, 325, 326, 326, 326, 326, 327, 328, 329, 329, 329, 329, 329, 330, 331, 331, 331, 331, 331, 331, 332, 333, 333, 333, 333, 334, 334, 334, 334, 334, 334, 335, 336, 336, 336, 336, 336, 336, 336, 337, 338, 339, 340, 341, 342, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 357, 358, 359, 360, 361, 362, 362, 362, 362, 362, 363, 363, 363, 363, 364, 364, 364, 364, 365, 366, 367, 367, 367, 368, 369, 370, 370, 371, 372, 373, 374, 374, 375, 376, 377, 378, 379, 380, 381, 382, 382, 383, 384, 385, 386, 386, 387, 387, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 407, 407, 407, 407, 407, 408, 408, 408, 408, 408, 409, 409, 409, 410, 410, 410, 410, 410, 411, 411, 411, 411, 412, 413, 413, 413]\n","tok_start_pos : 15\n","tok_end_pos : 21\n","answer tokens : ['cast', '##ration', '-', 'resistant', 'prost', '##ate', 'cancer']\n","[Docspan(start=0, length=370), Docspan(start=128, length=370), Docspan(start=256, length=370), Docspan(start=384, length=345)]\n","<__main__.InputFeatures object at 0x7fad70c0d900>\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-95-f34ae73094e8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconvert_examples_to_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFullTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/cose474_final/vocab_biobert_large_cased.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-93-fe9fbd23a5a9>\u001b[0m in \u001b[0;36mconvert_examples_to_features\u001b[0;34m(examples, tokenizer, max_seq_len, doc_stride, max_query_len, is_training, output_fn)\u001b[0m\n\u001b[1;32m    129\u001b[0m       )\n\u001b[1;32m    130\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m       \u001b[0moutput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m       \u001b[0munique_id\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"]}]}]}