{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP0HMe9AdzAt3wsZstwZCJz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### setting"],"metadata":{"id":"QB5XanEG-UW2"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P9ZsF12c9kfj","executionInfo":{"status":"ok","timestamp":1700455693536,"user_tz":-540,"elapsed":24943,"user":{"displayName":"이태후","userId":"03468433647806446322"}},"outputId":"d5e3588a-7f8b-4e2a-8f54-8556498fa3d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install -q transformers"],"metadata":{"id":"wK2z5YI2kB8u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### tokenization"],"metadata":{"id":"h21bQ-ZG-REt"}},{"cell_type":"code","source":["# useful functions for data cleansing\n","\n","import unicodedata\n","\n","def decode_bytes(text):\n","\n","  '''check for bytes within text and decode them'''\n","\n","  if isinstance(text, str):\n","    return text\n","  elif isinstance(text, bytes):\n","    return text.decode(\"utf-8\", \"ignore\")\n","  else:\n","    raise ValueError(f\"unsupported string type: {text}\")\n","\n","def whitespace_tokenize(text):\n","\n","  '''strip text and split them'''\n","\n","  text = text.strip()\n","  if not text:\n","    return []\n","  tokens = text.split()\n","  return tokens\n","\n","def is_control(char):\n","\n","  ''' detect control characters except whitespace '''\n","\n","  if unicodedata.category(char).startswith('C'):\n","    if char == '\\t' or char == '\\n' or char == '\\r': # category starts with C but is whitespace\n","      return False\n","    return True\n","  else:\n","    return False\n","\n","def is_whitespace(char):\n","\n","  ''' check whether character is whitespace '''\n","\n","  if char == ' ' or char == '\\t' or char == '\\n' or char == '\\r':\n","    return True\n","  elif unicodedata.category(char) == \"Zs\": # \"space separator\" category\n","    return True\n","  else:\n","    return False\n","\n","def is_punc(char):\n","\n","  ''' check whether character is a punctuation '''\n","  # ~32 : 각종 공백문자, 48~57 : 숫자, 65~90 : 대문자 알파벳, 97~122 : 소문자 알파벳, 126~ : 이상한 기호들\n","  if (ord(char) >= 33 and ord(char) <= 47) or (ord(char) >= 58 and ord(char) <= 64):\n","    return True\n","  elif (ord(char) >= 91 and ord(char) <= 96) or (ord(char) > 123 and ord(char) <= 126):\n","    return True\n","  elif unicodedata.category(char).startswith(\"P\"): # punctuation\n","    return True\n","  else:\n","    return False"],"metadata":{"id":"_-a5ehob-M3v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import collections\n","\n","def load_vocab(vocab_file):\n","\n","  ''' load vocab file as dictionary '''\n","\n","  vocab_dict = collections.OrderedDict()\n","  idx=0\n","\n","  with open(vocab_file, \"r\") as rf:\n","    tokens = rf.read().splitlines()\n","\n","  for token in tokens:\n","    token = decode_bytes(token)\n","    token = token.strip()\n","    vocab_dict[token] = idx\n","    idx += 1\n","\n","  return vocab_dict"],"metadata":{"id":"EABAro7d37Ar"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BasicTokenizer(object):\n","  def __init__(self, do_lower_case=True):\n","    self.do_lower_case = do_lower_case\n","\n","  def clean_text(self, text):\n","\n","    ''' skip invalid characters, convert all whitespaces into single space, and return text '''\n","\n","    output = []\n","    for char in text:\n","      # check if char is NULL,�(unrecognizable), or control character\n","      if ord(char) == 0 or ord(char) == 65533 or is_control(char):\n","        continue\n","      if is_whitespace(char):\n","        output.append(\" \") # all whitespace -> \" \"\n","      else:\n","        output.append(char)\n","    return \"\".join(output)\n","\n","  def run_split_on_punc(self, text):\n","\n","    ''' split text based on punctuations '''\n","\n","    output = []\n","    token = ''\n","    for char in list(text):\n","      if is_punc(char):\n","        output.append(token)\n","        output.append(char)\n","        token = ''\n","      else:\n","        token += char\n","\n","    if len(token) > 0:\n","      output.append(token)\n","\n","    return output\n","\n","  def tokenize(self, text):\n","    text = decode_bytes(text)\n","    text = self.clean_text(text)\n","\n","    orig_tokens = whitespace_tokenize(text) # 공백문자 단위로 분리\n","    split_tokens = []\n","\n","    for token in orig_tokens: # 토큰별로 punctuation 있을 경우 분리\n","      if self.do_lower_case:\n","        token = token.lower()\n","      split_tokens.extend(self.run_split_on_func(token))\n","\n","    return split_tokens"],"metadata":{"id":"qinZXvskH4a6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class WordpieceTokenizer(object):\n","  def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n","    self.vocab = vocab\n","    self.unk_token = unk_token\n","    self.max_input_chars_per_word = max_input_chars_per_word\n","\n","  def tokenize(self, text):\n","    ''' input : a single token (via BasicTokenizer), output : wordpiece tokens\n","\n","    wordpiece merges characters by \"score = freq(pair) / freq(first)*freq(second)\".\n","    pairs that frequently appear together are merged, but not if each element also appears frequently\n","\n","    ex) unable : \"un\", \"##able\"\n","    ex) hugging : \"hugging\" '''\n","\n","    text = decode_bytes(text)\n","\n","    output_tokens = []\n","    for token in whitespace_tokenize(text):\n","      chars = list(token)\n","      # 너무 긴 단어 (>200)는 unk 처리\n","      if len(chars) > self.max_input_chars_per_word:\n","        output_tokens.append(self.unk_token)\n","        continue\n","\n","      is_bad = False\n","      start = 0\n","      sub_tokens = []\n","\n","      # ex. unfriendly : length 10\n","      while start < len(chars): # 0~9\n","        end = len(chars) # 10\n","        cur_substr = None\n","        while start < end:\n","          substr = \"\".join(chars[start:end]) # unfriendly, unfriendl, unfriend, ... , un\n","          if start > 0:\n","            substr = \"##\" + substr # indicates subword\n","          if substr in self.vocab: # \"unfriendly\" not in vocab / ... \"un\" in vocab\n","            cur_substr = substr\n","            break\n","          end -= 1\n","        if cur_substr is None:\n","          is_bad = True\n","          break\n","        sub_tokens.append(cur_substr)\n","\n","        start = end # subword 떨어져나간 경우 (un) 끝자리->시작자리 (f)\n","\n","      if is_bad: # character 한개도 vocab에 없는 경우\n","        output_tokens.append(self.unk_token)\n","      else:\n","        output_tokens.extend(sub_tokens)\n","\n","    return output_tokens\n","\n"],"metadata":{"id":"1kgEsgaQnrCa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FullTokenizer(object):\n","  def __init__(self, vocab_file, do_lower_case=True):\n","    self.vocab = load_vocab(vocab_file)\n","    self.vocab_reverse = {v:k for k,v in self.vocab.items()}\n","    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n","    self.wordpiece = WordpieceTokenizer(vocab=self.vocab)\n","\n","  def tokenize(self, text):\n","    split_tokens = []\n","    for token in self.basic_tokenizer.tokenize(text): # cleanse text, tokenize by punctuation\n","      for sub_token in self.wordpiece.tokenize(token): # tokenize tokens (subtokens)\n","        split_tokens.append(sub_token)\n","\n","    return split_tokens"],"metadata":{"id":"E9y_UXkh3yQC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### finetune biobert"],"metadata":{"id":"BJBzS902_U93"}},{"cell_type":"code","source":["from absl import flags\n","\n","flags.DEFINE_string(\"bert_config_file\", None, \"config json file corresponding to the pretrained BERT model\")\n","flags.DEFINE_string(\"vocab_File\", None, \"vocab file on which BERT was trained\")\n","flags.DEFINE_string(\"output_dir\", None, \"output dir for model checkpoints\")\n","flags.DEFINE_string(\"train_file\", None, \"squad-formatted json for training. E.g., train-v1.1.json\")\n","flags.DEFINE_string(\"predict_file\", None, \"squad-formatted json for predictions. E.g., dev(test)-v1.1.json\")\n","flags.DEFINE_string(\"init_checkpoint\", None, \"initial checkpoint (usually from a pretrained BERT model)\")\n","\n","flags.DEFINE_bool(\"do_lower_case\", True, \"whether to lower-case input text. True for lower case.\")\n","flags.DEFINE_bool(\"do_train\", True, \"whether to run training\")\n","flags.DEFINE_bool(\"do_predict\", True, \"whether to run eval on the dev set\")\n","flags.DEFINE_bool(\"use_tpu\", False, \"whether to use TPU or GPU/CPU\")\n","flags.DEFINE_bool(\"verbose_logging\", False, \"whether to print all warnings during data processing. Other warnings are printed by default.\")\n","\n","flags.DEFINE_integer(\"max_seq_len\", 384, \"maximum input sequence after WordPiece tokenization. Will be padded if shorter, truncated if longer.\")\n","flags.DEFINE_integer(\"doc_stride\", 128, \"when splitting up a long document into chunks, how much stride to take between chunks\")\n","flags.DEFINE_integer(\"max_query_len\", 64, \"maximum query sequence after WordPiece tokenization. Will be truncated if longer.\")\n","flags.DEFINE_integer(\"predict_batch_size\", 8, \"total batch size for predictions\")\n","flags.DEFINE_integer(\"save_checkpoint_step\", 1000, \"how often to save model checkpoints\")\n","flags.DEFINE_integer(\"iterations_per_loop\", 1000, \"how many steps to make in each estimator call\")\n","flags.DEFINE_integer(\"n_best_size\", 20, \"how many n-best predictions to generate in nbest_predictions.json output file\")\n","flags.DEFINE_integer(\"max_answer_len\", 30, \"maximum length of generated answer\")\n","\n","flags.DEFINE_float(\"learning_rate\", 5e-5, \"initial learning rate for Adam optimizer\")\n","flags.DEFINE_float(\"num_train_epochs\", 3.0, \"number of training epochs\")\n","flags.DEFINE_float(\"warmup_proportion\", 0.1, \"proportion of training for linear lr warmup. E.g., 0.1 refers to 10% of training\")\n","flags.DEFINE_float(\"null_score_diff_threshold\", 0.0, \"to predict null if (null_score - best_non_null) > threshold\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M6iOQ_ARlZpZ","executionInfo":{"status":"ok","timestamp":1700455752692,"user_tz":-540,"elapsed":525,"user":{"displayName":"이태후","userId":"03468433647806446322"}},"outputId":"d5ed11b1-103e-4545-ec8d-2c4d869e2715"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<absl.flags._flagvalues.FlagHolder at 0x7d147f872770>"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# import sys\n","# sys.path.insert(1, '/content/drive/MyDrive/biobert')"],"metadata":{"id":"5INmE5aiyn-b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SquadExample(object):\n","  def __init__(self, qas_id, question_text, doc_tokens, origin_answer_text=None, start_pos=None, end_pos=None, is_impossible=False):\n","    self.qas_id = qas_id\n","    self.question_text = question_text\n","    self.doc_tokens = doc_tokens\n","    self.origin_answer_text = origin_answer_text\n","    self.start_pos = start_pos\n","    self.end_pos = end_pos\n","    self.is_impossible = is_impossible\n","\n","  def __str__(self):\n","    return self.__repr__()\n","\n","  def __repr__(self):\n","    s = f\"qas_id: {decode_bytes(self.qas_id)}\" # check if text is in str, bytes or unicode and convert\n","    s += f\", question_text: {decode_bytes(self.question_text)}\"\n","    s += f\", doc_tokens: {[' '.join(self.doc_tokens)]}\"\n","\n","    if self.start_pos:\n","      s += f\", start_pos: {self.start_pos}, end_pos: {self.end_pos}, is_impossible: {self.is_impossible}\"\n","\n","    return s\n"],"metadata":{"id":"CAu4TqjuvWVJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import tensorflow as tf\n","\n","class InputFeatures(object):\n","  def __init(self, unique_id, example_index, doc_span_index, tokens, token_to_origin_map, token_is_max_context,\n","             input_ids, input_mask, segment_ids, start_pos=None, end_pos=None, is_impossible=None):\n","    self.unique_id = unique_id\n","    self.example_index = example_index\n","    self.doc_span_index = doc_span_index\n","    self.tokens = tokens\n","    self.token_to_origin_map = token_to_origin_map\n","    self.token_is_max_context = token_is_max_context\n","    self.input_ids = input_ids\n","    self.input_mask = input_mask\n","    self.segment_ids = segment_ids\n","    self.start_pos = start_pos\n","    self.end_pos = end_pos\n","    self.is_impossible = is_impossible"],"metadata":{"id":"mIS-ohuH7TiL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def read_squad_examples(input_file, is_training):\n","  \"\"\"Read a squad json file into a list of SquadExample\"\"\"\n","\n","  with open(input_file, \"r\") as rf:\n","    input_data = json.load(rf)[\"data\"][0][\"paragraphs\"]\n","\n","  examples = []\n","  for entry in input_data:\n","    doc_tokens = []\n","    char_to_word_offset = []\n","    prev_is_whitespace = True\n","\n","    context = entry[\"context\"]\n","    for c in context:\n","      if is_whitespace(c):\n","        prev_is_whitespace=True\n","      else:\n","        if prev_is_whitespace: # c comes after whitespace -> add as new token\n","          doc_tokens.append(c)\n","        else:\n","          doc_tokens[-1] += c # c comes after c -> attach to recent token\n","        prev_is_whitespace=False\n","      char_to_word_offset.append(len(doc_tokens) -1) # \"I went home\" -> [0,0,1,1,1,1,1,2,2,2,2]\n","\n","    qas = entry[\"qas\"]\n","    for qa in qas:\n","      qas_id = qa[\"id\"]\n","      question_text = qa[\"question\"]\n","      start_pos = None\n","      end_pos = None\n","      origin_answer_text = None\n","      is_impossible = False\n","\n","      if is_training:\n","        if len(qa[\"answers\"]) != 1:\n","          raise ValueError(\"Each question should have exactly 1 answer.\")\n","\n","        answer = qa[\"answers\"][0]\n","        origin_answer_text = answer[\"text\"] #\"Bazex syndrome\"\n","        answer_offset = answer[\"answer_start\"] #93\n","        answer_length = len(origin_answer_text)\n","        start_pos = char_to_word_offset[answer_offset] # 몇번째 단어부터\n","        end_pos = char_to_word_offset[answer_offset + answer_length-1] # 몇번째 단어까지\n","\n","        # check if answer text is extractable from context (아닌 경우 건너뜀)\n","        answer_from_context = \" \".join(doc_tokens[start_pos:end_pos+1]) # context에서 주어진 범위로 인덱싱한 정답\n","        cleaned_answer = \" \".join(whitespace_tokenize(origin_answer_text)) # 실제 정답 (strip 및 split)\n","\n","        if answer_from_context.find(cleaned_answer) == -1: # context에 정답이 들어있지 않은 경우\n","          tf.logging.warning(f\"Could not find answer from context. {answer_from_context} vs {cleaned_answer}\")\n","          continue\n","\n","      else: # inference용\n","        start_pos = -1\n","        end_pos = -1\n","        origin_answer_text = \"\"\n","\n","      example = SquadExample(qas_id=qas_id, question_text=question_text, doc_tokens=doc_tokens,\n","                              origin_answer_text=origin_answer_text, start_pos=start_pos, end_pos=end_pos,\n","                              is_impossible=is_impossible)\n","      examples.append(example)\n","\n","  return examples"],"metadata":{"id":"Jb-H6ed4BC7Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["examples = read_squad_examples('/content/drive/MyDrive/cose474_final/trainset_bioasq.json', is_training=True)"],"metadata":{"id":"N2Pa84IpUCGT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(examples[0])\n","print(examples[0].question_text) # each element in \"read_squad_examples\" is a SquadExample object.\n","print(type(examples[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vk77r6R7_ETU","executionInfo":{"status":"ok","timestamp":1700471317178,"user_tz":-540,"elapsed":4,"user":{"displayName":"이태후","userId":"03468433647806446322"}},"outputId":"275dfc81-6239-45eb-dbae-c5ba6e732027"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["qas_id: 56bc751eac7ad10019000013_001, question_text: Name synonym of Acrokeratosis paraneoplastica., doc_tokens: ['Psoriasiform dermatitis in a case of newly diagnosed locally advanced pyriform sinus tumour: Bazex syndrome revisited. Acrokeratosis paraneoplastica of Bazex is a rare but important paraneoplastic dermatosis, usually manifesting as psoriasiform rashes over the acral sites. It often precedes diagnosis of the associated malignancy, usually that of upper aerodigestive tract squamous cell carcinoma. We present the case of a patient with a newly diagnosed pyriform sinus tumour and associated acrokeratosis paraneoplastica. To the best of our knowledge, this is the first reported case in the local literature.'], start_pos: 13, end_pos: 14, is_impossible: False\n","Name synonym of Acrokeratosis paraneoplastica.\n","<class '__main__.SquadExample'>\n"]}]},{"cell_type":"code","source":["def convert_examples_to_features(examples, tokenizer, max_seq_len, doc_stride, max_query_len, is_training, output_fn):\n","  unique_id = 1000000000\n","\n","  for idx, example in enumerate(examples):\n","    query_tokens = tokenizer.tokenize(example.question_text)\n","\n","    if len(query_tokens) > max_query_len: # cut query if it exceeds maximum token length\n","      query_tokens = query_tokens[:max_query_len]\n","\n","    token_to_orig_idx = []\n","    orig_to_token_idx = []\n","    all_doc_tokens = []\n","\n","    for i, token in enumerate(example.doc_tokens): # whitespace로만 분리된 tokens\n","      orig_to_token_idx.append(len(all_doc_tokens))\n","      sub_tokens = tokenizer.tokenize(token) # tokenize into subwords\n","      for sub_token in sub_tokens:\n","        token_to_orig_idx.append(i)\n","        all_doc_tokens.append(sub_token)"],"metadata":{"id":"fOOxoauhVffT"},"execution_count":null,"outputs":[]}]}